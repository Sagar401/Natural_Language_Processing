{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disaster_Tweets_NLP_Final_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qE6l7WrmqO0x",
        "IdrbH1xiro6d",
        "KTnfUqNcsTWE",
        "zs_gLA77twFv",
        "mlTK9twGuBPm",
        "wQ8ed3F6vbGz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUVYoowYY0bU",
        "colab_type": "text"
      },
      "source": [
        "#**Real or Not? Binary Classification of Disaster Tweets**\n",
        "Kaggle Competition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYep4GayYvRx",
        "colab_type": "text"
      },
      "source": [
        "##Loading all necessary liraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAWqRcnaZRE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# XGBoost\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "# sklearn \n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvhmsa7uZQe2",
        "colab_type": "text"
      },
      "source": [
        "##Reading Training and Test Dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDp-A-3pZhcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR_fQXADZZ7N",
        "colab_type": "text"
      },
      "source": [
        "##Basic EDA - Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZwbHYdpZkAh",
        "colab_type": "code",
        "outputId": "98db7417-16fe-4eed-86a5-c0bb33c9b900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#Looking at first few rows of dataset\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y7hwc5OZlHU",
        "colab_type": "code",
        "outputId": "e46f880b-afe4-4814-cab7-c022e9a4c345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Training Dataset Size:\",train_df.shape)\n",
        "print(\"Test Dataset Size:\",test_df.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset Size: (7613, 5)\n",
            "Test Dataset Size: (3263, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pagQ63RPZlFM",
        "colab_type": "code",
        "outputId": "b1d359ee-09d8-4a74-99be-b34723805525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#missing values in training dataset\n",
        "train_df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id             0\n",
              "keyword       61\n",
              "location    2533\n",
              "text           0\n",
              "target         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wYRUVjNZu55",
        "colab_type": "text"
      },
      "source": [
        "###Target column Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kMm7JJaZky5",
        "colab_type": "code",
        "outputId": "ebb09da6-d802-4571-d004-3a98e639f809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_df['target'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8KbOCINZkvB",
        "colab_type": "code",
        "outputId": "1966a4af-585d-4e30-c554-6f8a8f699039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#exlporing location column\n",
        "train_df['location'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "USA                          104\n",
              "New York                      71\n",
              "United States                 50\n",
              "London                        45\n",
              "Canada                        29\n",
              "                            ... \n",
              "IDN                            1\n",
              "Winnipeg, MB, Canada           1\n",
              "Cleveland, Ohio                1\n",
              "Yadkinville, NC                1\n",
              "Wolverhampton/Brum/Jersey      1\n",
              "Name: location, Length: 3341, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlnv5SbZa7xd",
        "colab_type": "text"
      },
      "source": [
        "##**Data Cleaning**\n",
        "\n",
        "\n",
        "*   Making all uppercase to lowercase\n",
        "*   Removing noise from tweets\n",
        "\n",
        "  *   URLs\n",
        "  *   HTML tags\n",
        "  *   emogis\n",
        "  *   Punctuation\n",
        "  *   New-Line\n",
        "  *   Removing Digits\n",
        "*   Tokenization: Converting normal text string into a list of tokens/words\n",
        "*   Stopwords removal(optional)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk4-ZDN4lTti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to remove noise from text\n",
        "def clean_text(text):\n",
        "    text = text.lower() #convert to lowercase to maintain standard flow between text\n",
        "    text = re.sub('\\[.*?\\]', '', text) #removing text in square brackets\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) #removing url\n",
        "    text = re.sub('<.*?>+', '', text) #removing html tags\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) #removing puntuations\n",
        "    text = re.sub('\\n', '', text)#removing new line from the text field\n",
        "    text = re.sub('\\w*\\d\\w*', '', text) #removing digits from the string\n",
        "    return text\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(lambda x : clean_text(x))\n",
        "test_df['text'] = test_df['text'].apply(lambda x : clean_text(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GjtNoIclamq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to remove emoji's\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                            \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "train_df['text']=train_df['text'].apply(lambda x: remove_emoji(x))\n",
        "test_df['text']=test_df['text'].apply(lambda x: remove_emoji(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QplrjMzKlwVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#removing alpha numeric values from text\n",
        "train_df['text'] = train_df['text'].str.replace('[^a-z A-Z]','')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZwInMdRL7No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adding new column with the count of words in a single row\n",
        "train_df['word_count'] = train_df['text'].str.split().map(len)\n",
        "\n",
        "#only using the rows with word count more than 0\n",
        "train_df = train_df[train_df['word_count'] > 0]\n",
        "\n",
        "#moving forward with 2 column from dataset , i.e., text and target\n",
        "train_df = train_df[[\"text\",\"target\"]]\n",
        "test_df = test_df[[\"text\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTnfUqNcsTWE",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Words- Countvectorizer Features\n",
        "Countvectorizer convert a collection of text documents to a matrix of token counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiC4PBBuJkd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "train_vectors = count_vectorizer.fit_transform(train_df['text'])\n",
        "test_vectors = count_vectorizer.transform(test_df[\"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs_gLA77twFv",
        "colab_type": "text"
      },
      "source": [
        "###TF-IDF Features\n",
        "(Term Frequency-Inverse Document Frequency) - Rescaling the frequency of words hows often they appear in all the documents\n",
        "\n",
        "**Term Frequency: is a scoring of the frequency of the word in the current document or text**\n",
        "\n",
        "TF = (Number of times term t appears in a document)/(Number of terms in a document)\n",
        "\n",
        "**Inverse Document Frequency: is a scoring of how rare the word is across documents**\n",
        "\n",
        "IDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t appeared in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7tZJtIDtnPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2))\n",
        "train_dfidf = tfidf.fit_transform(train_df['text'])\n",
        "test_dfidf = tfidf.transform(test_df['text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUvzjYXbt7kt",
        "colab_type": "text"
      },
      "source": [
        "##Building Text Classification Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlTK9twGuBPm",
        "colab_type": "text"
      },
      "source": [
        "###Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XDhpmp0uGI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting a simple Logistic Regression on Count-vectors\n",
        "clf = LogisticRegression(C=1.0)\n",
        "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNGdi9q1uKFM",
        "colab_type": "code",
        "outputId": "6800cf57-1c2b-4487-c90e-55e6f38019ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.62962963, 0.55050505, 0.61184211, 0.57837838, 0.71981058])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txcxoNeVudzS",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression with **Bag of words** features results, \n",
        "\n",
        "After 5th cross validation training F1 score is 71.9% "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdNRbw-zuKCs",
        "colab_type": "code",
        "outputId": "97dc8a40-8284-44b7-91d4-c3587893e39b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Fitting a simple Logistic Regression on TFIDF\n",
        "clf_tfidf = LogisticRegression(C=1.0)\n",
        "scores = model_selection.cross_val_score(clf_tfidf, train_dfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.61538462, 0.56688494, 0.60249554, 0.58574181, 0.71885522])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9R6ToV7uwKI",
        "colab_type": "text"
      },
      "source": [
        "Logistic Regression with **TF-IDF** Features results, \n",
        "\n",
        "after 5th cross validation training F1 score is 71.8%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ8ed3F6vbGz",
        "colab_type": "text"
      },
      "source": [
        "###Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukta0WLeuKAH",
        "colab_type": "code",
        "outputId": "95e35928-9a8f-4db9-efa1-3df97dd4deae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Fitting a simple Naive Bayes on Count-Vectors\n",
        "clf_NB = MultinomialNB()\n",
        "scores = model_selection.cross_val_score(clf_NB, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.65368332, 0.63370787, 0.68676471, 0.64526485, 0.73986014])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA5SJ-lIvxkI",
        "colab_type": "text"
      },
      "source": [
        "Naive Bayes Classifier with **Bag of words** features results, \n",
        "\n",
        "After 5th cross validation training F1 score is 73.9% "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hBVgVvyvn15",
        "colab_type": "code",
        "outputId": "095f4198-cca2-4868-e15a-17df9eddd571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Fitting a simple Naive Bayes on TFIDF\n",
        "clf_NB_TFIDF = MultinomialNB()\n",
        "scores = model_selection.cross_val_score(clf_NB_TFIDF, train_dfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\n",
        "scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.59486166, 0.58962693, 0.6356453 , 0.60170293, 0.74196208])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uj_JU7mxjdc",
        "colab_type": "text"
      },
      "source": [
        "Naive Bayes Classifier with **TF-IDF** features results,\n",
        "\n",
        "After 5th Cross Validation taining F1 Score is 74.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TjDnib01ZA6",
        "colab_type": "text"
      },
      "source": [
        "##Neural Network - Deep Learning\n",
        "\n",
        "Why not a standard network fit for this data challenge?\n",
        "\n",
        "\n",
        "*   Inputs, outputs can be of different lengths in different examples\n",
        "*   Doesn't share features learned across different position of text\n",
        "\n",
        "To overcome this we use Recurrent Neural Network, \n",
        "\n",
        "as it scans words from left to right, one drawback is that it only knows features from its left\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bVoU3twyBWc",
        "colab_type": "text"
      },
      "source": [
        "##Word Level - Vanilla RNN (RNN from Scratch) (Recurrent Neural Network)\n",
        "\n",
        "The idea is to create a word level RNN in python/numpy that will provide a baseline model for more complex Neural Networks architecture, and also to gain low level understanding of the working of RNN (Sequence model)\n",
        "\n",
        "Credits: \n",
        "\n",
        "Andrej Karpathy https://gist.github.com/karpathy/d4dee566867f8291f086: Minimal character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy. And the blog http://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n",
        "\n",
        "The deep learning book by Michael Nielsen particularly http://neuralnetworksanddeeplearning.com/chap6.html\n",
        "\n",
        "Andrew ng Deep learning course (Course 5) on Coursera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfARaugsy1x7",
        "colab_type": "text"
      },
      "source": [
        "**Steps Taken:**\n",
        "\n",
        "\n",
        "\n",
        "1.   Creating a vocabulary list of unique words from data to be later used in encoding each words into a one-hot vector using 1-k encoding. (k = len(vocab_list)\n",
        "2.   Initialize the RNN model parameters\n",
        "3.   Feedforward the training data(tweets) vectorized form into the network and calculate loss for that training example\n",
        "4.   Backpropagate through time and obtain the gradient parameters.\n",
        "5.   Gradient clipping to avoid exploding gradient problem\n",
        "6.   Select/iterating over different learning rate and calculating new model paramters\n",
        "7.   Repeating steps from 3-6 for some number of iterations that covers all training examples atleast 3 times\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZiAXic3yXYT",
        "colab_type": "code",
        "outputId": "4ef79eaa-c81f-4f32-cede-19cbd40fb9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#creating vocabulary list which will have uniqe word list from training data\n",
        "vocab_list = list(train_df['text'].str.split(' ',expand=True).stack().unique())\n",
        "total_words = list(train_df['text'].str.split(' ',expand=True).stack())\n",
        "\n",
        "vocab_list_size = len(vocab_list)\n",
        "total_words_len = len(total_words)\n",
        "\n",
        "print(\"Vocab size : \",vocab_list_size)\n",
        "print(\"Total words in data : \", total_words_len)\n",
        "\n",
        "#creating a dictionary that has an index for each of the unique words\n",
        "words_idx = { word:i for i, word in enumerate(vocab_list) }\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size :  16512\n",
            "Total words in data :  113521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ7zQJhT26wO",
        "colab_type": "code",
        "outputId": "2767546a-d8e2-42d4-dd64-d0535e3fd14d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#converting a single training sample to retreive from dictionary\n",
        "temp = train_df['text'].str.split().values[1]\n",
        "inputs = [words_idx[i] for i in temp]\n",
        "output = train_df['target'].values[1]\n",
        "print(\"original text:\",temp)\n",
        "print(\"Wordlist index:\",inputs)\n",
        "print(\"Target label:\",output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original text: ['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada']\n",
            "Wordlist index: [13, 14, 15, 16, 17, 18, 19]\n",
            "Target label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYh8fLTje50u",
        "colab_type": "code",
        "outputId": "0f2ea8f5-e8f3-445f-cfe8-eea65ff3c0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#hyperparameters\n",
        "learning_rate = 0.005\n",
        "n_a = hidden_size = 100\n",
        "n_x = vocab_list_size \n",
        "n_y = 2\n",
        "\n",
        "#model_parameters\n",
        "Waa = np.random.randn(hidden_size,hidden_size)*0.1\n",
        "Wax = np.random.randn(hidden_size,vocab_list_size)*0.1\n",
        "Wya = np.random.randn(2,hidden_size)*0.1\n",
        "ba = np.zeros((n_a,1))\n",
        "by = np.zeros((n_y,1))\n",
        "\n",
        "#spliting into training and validation\n",
        "temp_df = train_df\n",
        "train_df = train_df.iloc[:7000]\n",
        "validation_df = temp_df.iloc[7000:]\n",
        "\n",
        "print('The training set examples: %d' %(len(train_df)))\n",
        "print('The validation set examples: %d' %(len(validation_df)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The training set examples: 7000\n",
            "The validation set examples: 612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iu9NV_W6W7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#activation function\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aynFog1thdO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#feed-forward -- takes in the index of words in a example tweet and return the prediction\n",
        "def rnn_feedforward(input_data):\n",
        "  #initializing\n",
        "  xt,at = [], np.zeros((n_a,1))\n",
        "  for t in range(len(input_data)):\n",
        "    xt.append(np.zeros((n_x,1)))#encode in 1-k one hot-representation\n",
        "    xt[t][input_data[t]] = 1\n",
        "    at = np.tanh(np.dot(Waa,at)+np.dot(Wax,xt[t])+ba) #hidden state activation function\n",
        "    \n",
        "  yt = np.dot(Wya,at) + by \n",
        "  pred = softmax(yt) #softmax function for getting probability for binary classification\n",
        "  prediction = np.argmax(pred)\n",
        "  return prediction\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e14EmCeuNrp",
        "colab_type": "code",
        "outputId": "ceb64cee-c28e-4a8f-a286-c917bcdcbc15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "\n",
        "num_iterations = 5\n",
        "\n",
        "#memory variables for Adagrad\n",
        "mWaa, mWya, mWax = np.zeros_like(Waa), np.zeros_like(Wya), np.zeros_like(Wax)\n",
        "mby, mba = np.zeros_like(by), np.zeros_like(ba)\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  \n",
        "  idx = i%len(train_df)\n",
        "  example = train_df['text'].str.split().values[idx]\n",
        "  inputs = [words_idx[i] for i in example]\n",
        "  print(inputs)\n",
        "  targets = int(train_df['target'].values[idx])\n",
        "  \n",
        "  prediction = rnn_feedforward(inputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "[[0.56772971]\n",
            " [0.43227029]]\n",
            "prediction: 0\n",
            "[13, 14, 15, 16, 17, 18, 19]\n",
            "[[0.46812068]\n",
            " [0.53187932]]\n",
            "prediction: 1\n",
            "[12, 20, 21, 22, 23, 24, 25, 2, 26, 27, 28, 29, 30, 31, 32, 33, 23, 24, 25, 34, 2, 35]\n",
            "[[0.36159748]\n",
            " [0.63840252]]\n",
            "prediction: 1\n",
            "[37, 38, 39, 32, 34, 24, 40]\n",
            "[[0.43414214]\n",
            " [0.56585786]]\n",
            "prediction: 1\n",
            "[41, 42, 43, 6, 44, 45, 46, 47, 48, 49, 45, 39, 50, 51, 52, 53]\n",
            "[[0.6215141]\n",
            " [0.3784859]]\n",
            "prediction: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJN5SyxA7NMU",
        "colab_type": "text"
      },
      "source": [
        "Function that takes input as one row with text and target value, feedforward the network, calculates the cost which is loss of predicted vs actual Y/target.\n",
        "\n",
        "Then performs backpropagation and updates the gradient of all the parameters and returns it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5WvmR4uJ2NS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#feedforward and backpropagation\n",
        "def rnn_model(input_data,targets):\n",
        "  \n",
        "  xt,at = [],[]\n",
        "  at.append(np.zeros((n_a,1)))\n",
        "  loss = 0\n",
        "  \n",
        "  #feed forward\n",
        "  for t in range(len(input_data)):\n",
        "    xt.append(np.zeros((n_x,1)))\n",
        "    xt[t][input_data[t]] = 1\n",
        "    \n",
        "    at.append(np.tanh(np.dot(Waa,at[t])+np.dot(Wax,xt[t])+ba))\n",
        "    \n",
        "  yt = np.dot(Wya,at[-1]) + by\n",
        "  pred = np.exp(yt) / np.sum(np.exp(yt),axis=0)\n",
        "  \n",
        "  prediction = np.argmax(pred)\n",
        "  \n",
        "  #loss-cost\n",
        "  \n",
        "  y = np.zeros((2,1))\n",
        "  y[targets] = 1\n",
        "  loss = -np.log(np.sum(pred*y,axis = 0)) #cross-entropy loss\n",
        "  \n",
        "  #backpropagation through time\n",
        "  dWaa, dWya, dWax = np.zeros_like(Waa), np.zeros_like(Wya), np.zeros_like(Wax)\n",
        "  dby, dba = np.zeros_like(by), np.zeros_like(ba)\n",
        "  \n",
        "  dy = pred - y\n",
        "  dWya = np.dot(dy,at[-1].transpose())\n",
        "  dby = np.copy(dy)\n",
        "  \n",
        "  dat = np.dot(Wya.transpose(),dy)\n",
        "  dtanh = (1-at[-1]*at[-1]) * dat\n",
        "  \n",
        "  dWax = np.dot(dtanh, xt[-1].transpose())\n",
        "  dWaa = np.dot(dtanh, at[-2].transpose())\n",
        "  dba = np.copy(dtanh)\n",
        "  da_next = np.dot(Waa.transpose(),dtanh)\n",
        "  for t in reversed(range(len(input_data)-2)):\n",
        "    \n",
        "    dat = np.copy(da_next)\n",
        "    dtanh = (1- at[t+1] * at[t+1]) * dat\n",
        "    \n",
        "    dWax += np.dot(dtanh, xt[t].transpose())\n",
        "    dWaa += np.dot(dtanh, at[t].transpose())\n",
        "    dba += np.copy(dtanh)\n",
        "    da_next = np.dot(Waa.transpose(),dtanh)\n",
        "    \n",
        "  for dparams in [dWaa,dWax,dba,dby,dWya]:\n",
        "    np.clip(dparams,-5,5,out=dparams)\n",
        "    \n",
        "  return loss, dWaa, dWax, dba, dby, dWya\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3N0BoPp702x",
        "colab_type": "text"
      },
      "source": [
        "Feeding training data into the network to retrieve the gradient and using Adagrad optimizer to perform the gradient descent. \n",
        "\n",
        "And we repeat this process for all the training examples and for n epochs\n",
        "\n",
        "**Adagrad Optimizer** : Adaptively scales the learning rate with respect to the accumulated square gradient at each iteration in each dimention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JvLXdlxeHfM",
        "colab_type": "code",
        "outputId": "22e07a18-7df5-4804-93da-5d481272c891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "\n",
        "#select any number of iteration for start\n",
        "num_iterations = 21000\n",
        "\n",
        "#memory variables for Adagrad optimizer - backprop\n",
        "mWaa, mWya, mWax = np.zeros_like(Waa), np.zeros_like(Wya), np.zeros_like(Wax)\n",
        "mby, mba = np.zeros_like(by), np.zeros_like(ba)\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  idx = i%len(train_df)\n",
        "  example = train_df['text'].str.split().values[idx]\n",
        "  inputs = [words_idx[i] for i in example]\n",
        "  targets = int(train_df['target'].values[idx])\n",
        "  \n",
        "  #prediction = rnn_feedforward(inputs,targets)\n",
        "  loss,dWaa, dWax, dba, dby, dWya = rnn_model(inputs,targets)\n",
        "  \n",
        "  #Adagrad optimizer\n",
        "  #performing paramete update with Adagrad\n",
        "  for param, dparam, mem in zip([Waa, Wax, Wya, ba, by],\n",
        "                                [dWaa, dWax, dWya, dba, dby],\n",
        "                                [mWaa, mWax, mWya, mba, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem+1e-8) #adagrad update\n",
        "  \n",
        "  # validation accuracy\n",
        "  # using for loop instead of vectorization\n",
        "  if i % 700 == 0:\n",
        "    predictions = []\n",
        "    count=0\n",
        "    actual_targets= validation_df['target'].tolist()\n",
        "    for j in range(len(validation_df)):\n",
        "        example = validation_df['text'].str.split().values[j]\n",
        "        inputs = [words_idx[l] for l in example]\n",
        "        predictions.append(rnn_feedforward(inputs))\n",
        "          \n",
        "    for y, y_hat in zip(actual_targets, predictions):\n",
        "        if y==y_hat:\n",
        "            count+=1\n",
        "    print('The validation_accuracy after iterations:%d is %d'%(i,(count/len(validation_df))*100))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The validation_accuracy after iterations:0 is 51\n",
            "The validation_accuracy after iterations:700 is 47\n",
            "The validation_accuracy after iterations:1400 is 50\n",
            "The validation_accuracy after iterations:2100 is 48\n",
            "The validation_accuracy after iterations:2800 is 62\n",
            "The validation_accuracy after iterations:3500 is 69\n",
            "The validation_accuracy after iterations:4200 is 69\n",
            "The validation_accuracy after iterations:4900 is 69\n",
            "The validation_accuracy after iterations:5600 is 73\n",
            "The validation_accuracy after iterations:6300 is 72\n",
            "The validation_accuracy after iterations:7000 is 72\n",
            "The validation_accuracy after iterations:7700 is 70\n",
            "The validation_accuracy after iterations:8400 is 67\n",
            "The validation_accuracy after iterations:9100 is 72\n",
            "The validation_accuracy after iterations:9800 is 74\n",
            "The validation_accuracy after iterations:10500 is 76\n",
            "The validation_accuracy after iterations:11200 is 75\n",
            "The validation_accuracy after iterations:11900 is 73\n",
            "The validation_accuracy after iterations:12600 is 76\n",
            "The validation_accuracy after iterations:13300 is 74\n",
            "The validation_accuracy after iterations:14000 is 75\n",
            "The validation_accuracy after iterations:14700 is 74\n",
            "The validation_accuracy after iterations:15400 is 74\n",
            "The validation_accuracy after iterations:16100 is 75\n",
            "The validation_accuracy after iterations:16800 is 74\n",
            "The validation_accuracy after iterations:17500 is 75\n",
            "The validation_accuracy after iterations:18200 is 75\n",
            "The validation_accuracy after iterations:18900 is 74\n",
            "The validation_accuracy after iterations:19600 is 74\n",
            "The validation_accuracy after iterations:20300 is 74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIfcCuNk9DWf",
        "colab_type": "text"
      },
      "source": [
        "##Results Summary:\n",
        "The validation accuracy arrives closely to 76% (better than randomly guessing). Tried 2 sets of learning rate values with 0.001 accuracy was 70% whereas with learning rate 0.005 accuracy boosted to 76%. \n",
        "\n",
        "I have not experimented with other hyperparameters or tried any high level/complex data cleaning. Still, it seems that my RNN is learning association between words that helps to classify the tweets.\n",
        "\n",
        "Future scope:\n",
        "we can try different network architectures \n",
        "training model for longer iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfhu-vsj1W2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saving matrix values for parameters which includes weights and biases for later use to predict test data\n",
        "# saving the model\n",
        "import pickle\n",
        "filename = 'rnn_model_v1.pkl'\n",
        "with open(filename, \"wb\") as f:\n",
        "    pickle.dump((Waa, Wax, ba, by, Wya ), f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjG3m6EBJxcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading the saved model\n",
        "import pickle\n",
        "filename = 'rnn_model_v1.pkl'\n",
        "with open(filename, \"rb\") as f:\n",
        "    Waa, Wax, ba, by, Wya  = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8QnItPSaLq4",
        "colab_type": "code",
        "outputId": "9b233914-2dc4-47ae-f3c7-9763f8ec7311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#checking if parameters loaded in correctly\n",
        "#using loaded model\n",
        "count = 0\n",
        "for j in range(len(validation_df)):\n",
        "  \n",
        "  example = validation_df['text'].str.split().values[j]\n",
        "  inputs = [words_idx[l] for l in example]\n",
        "  predictions.append(rnn_feedforward(inputs))\n",
        "          \n",
        "for y, y_hat in zip(actual_targets, predictions):\n",
        "  if y==y_hat:\n",
        "    count+=1\n",
        "print(\"Correctly Predicted:\",count)\n",
        "print(\"Total Data in validation set:\",len(validation_df))\n",
        "print('The validation_accuracy is',(count/len(validation_df))*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correctly Predicted: 453\n",
            "Total Data in validation set: 612\n",
            "The validation_accuracy is 74.01960784313727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3srMga22Dm2B",
        "colab_type": "text"
      },
      "source": [
        "##Pushing Files into GitHub from google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr0VGFr9qvrq",
        "colab_type": "code",
        "outputId": "bf27e9ac-6407-4783-b5bf-045a91ffcebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git init"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized empty Git repository in /content/.git/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuNyUAX-Brt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git config -- globaluser.email \"sagardaswani401@gmail.com\"\n",
        "!git config -- globaluser.name \"Sagar401\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB9XTBslDAGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git add -A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzyRoOi-DMsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git commit -m \"first commit\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkdZFmYHDQ4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}